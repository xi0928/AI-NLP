{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'D:\\\\AI\\\\data\\\\notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) + beta_regul * tf.nn.l2_loss(weights)\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#设置超参数Beta为10^-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 20.129984\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 12.8%\n",
      "Minibatch loss at step 500: 2.547391\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 1000: 1.816388\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 77.7%\n",
      "Minibatch loss at step 1500: 0.962926\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 2000: 0.799073\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 80.4%\n",
      "Minibatch loss at step 2500: 0.730822\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 80.9%\n",
      "Minibatch loss at step 3000: 0.798177\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 81.6%\n",
      "Test accuracy: 88.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#探索一下超参数的最优值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (logistic)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#多一层relu隐藏层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 696.433472\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 29.7%\n",
      "Minibatch loss at step 500: 200.254868\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 1000: 114.880768\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 1500: 68.106483\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 2000: 41.220245\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 84.5%\n",
      "Minibatch loss at step 2500: 25.021698\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 3000: 15.505817\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 86.3%\n",
      "Test accuracy: 93.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:    \n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(num_steps):\n",
    "      # Pick an offset within the training data, which has been randomized.\n",
    "      # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "      # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "      # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "      # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "      # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (1-layer net)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 377.225647\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 26.2%\n",
      "Minibatch loss at step 5: 134.668640\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 69.8%\n",
      "Minibatch loss at step 10: 31.509201\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 70.3%\n",
      "Minibatch loss at step 15: 12.981134\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 73.6%\n",
      "Minibatch loss at step 20: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 25: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 30: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 35: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 40: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 45: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 50: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 55: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 60: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 65: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 70: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 75: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 80: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 85: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 90: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 95: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.6%\n",
      "Test accuracy: 82.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 100\n",
    "num_batches = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = ((step % num_batches) * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 5 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "  logits = tf.matmul(drop1, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 486.189148\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 24.2%\n",
      "Minibatch loss at step 5: 135.959656\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 63.5%\n",
      "Minibatch loss at step 10: 19.929626\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 66.9%\n",
      "Minibatch loss at step 15: 0.052234\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 66.7%\n",
      "Minibatch loss at step 20: 0.936623\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 25: 1.780639\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 65.7%\n",
      "Minibatch loss at step 30: 16.265604\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 68.8%\n",
      "Minibatch loss at step 35: 4.513223\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.5%\n",
      "Minibatch loss at step 40: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.7%\n",
      "Minibatch loss at step 45: 1.269597\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 67.4%\n",
      "Minibatch loss at step 50: 3.236887\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 68.4%\n",
      "Minibatch loss at step 55: 0.126954\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 65.8%\n",
      "Minibatch loss at step 60: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.4%\n",
      "Minibatch loss at step 65: 0.278099\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.6%\n",
      "Minibatch loss at step 70: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.3%\n",
      "Minibatch loss at step 75: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.3%\n",
      "Minibatch loss at step 80: 0.859320\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.4%\n",
      "Minibatch loss at step 85: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.8%\n",
      "Minibatch loss at step 90: 0.363165\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.9%\n",
      "Minibatch loss at step 95: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.7%\n",
      "Minibatch loss at step 100: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.7%\n",
      "Test accuracy: 75.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_batches = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = step % num_batches\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 5 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#按理说，与Problem2的Test accuracy相比，应该是增大的，也就是dropout一些神经元后，可以一定程度上减小Overfit，但Test accuracy结果是降低的\n",
    "#可能是神经网络太简单的原因"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 100\n",
    "beta_regul = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    ) #stddev为了使神经网络更快收敛\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "  logits = tf.matmul(lay2_train, weights3) + biases3\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3))\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.65, staircase=True) #应用于学习率的指数衰减函数\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay2_test, weights3) + biases3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.277215\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 30.7%\n",
      "Minibatch loss at step 500: 0.919247\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 1000: 0.859447\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 1500: 0.560781\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 2000: 0.518377\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 2500: 0.458424\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 3000: 0.562293\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 3500: 0.559631\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 4000: 0.486010\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 4500: 0.450961\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 5000: 0.509986\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 5500: 0.512534\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 6000: 0.484564\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 6500: 0.410194\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 7000: 0.489507\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 7500: 0.418173\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 8000: 0.598718\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 8500: 0.440046\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 9000: 0.499920\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.0%\n",
      "Test accuracy: 95.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 9001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#再加一层神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 256\n",
    "num_hidden_nodes3 = 128\n",
    "beta_regul = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_hidden_nodes3], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "  weights4 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes3, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes3)))\n",
    "  biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "  lay3_train = tf.nn.relu(tf.matmul(lay2_train, weights3) + biases3)\n",
    "  logits = tf.matmul(lay3_train, weights4) + biases4\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) + \\\n",
    "    beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3)+ tf.nn.l2_loss(weights4))\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 800, 0.8, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights4) + biases4)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay3_test, weights4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.474672\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 35.1%\n",
      "Minibatch loss at step 500: 1.063434\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 1000: 0.940260\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 1500: 0.613848\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 2000: 0.557993\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 2500: 0.506357\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 3000: 0.583875\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 3500: 0.559813\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 4000: 0.488061\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 4500: 0.450292\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 5000: 0.493456\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 5500: 0.491817\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 6000: 0.465181\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 6500: 0.405030\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 7000: 0.468478\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 7500: 0.417529\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 8000: 0.583290\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 8500: 0.408460\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 9000: 0.447259\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 9500: 0.390682\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 10000: 0.369953\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 10500: 0.417652\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 11000: 0.321976\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 11500: 0.357216\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 12000: 0.446934\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 12500: 0.354819\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 13000: 0.423874\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 13500: 0.318167\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 14000: 0.374984\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 14500: 0.460603\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 15000: 0.333496\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 15500: 0.382074\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 16000: 0.297344\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 16500: 0.334770\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 17000: 0.328902\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 17500: 0.212865\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 18000: 0.296801\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 18500: 0.349984\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 19000: 0.280068\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 19500: 0.355057\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.0%\n",
      "Test accuracy: 96.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20000\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
